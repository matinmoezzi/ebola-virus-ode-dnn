{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sysODE_keras.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM+cQ1MPeqQJFcLogVc7mvU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matinmoezzi/ebola-virus-ode-dnn/blob/main/sysODE_keras_Adam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8ndCajbQcN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AI78gy9eszH"
      },
      "source": [
        "train_size = 10000\n",
        "test_size = 500\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrqymngPfBMq"
      },
      "source": [
        "x_min = 0\n",
        "x_max = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3aG8n1neMI6"
      },
      "source": [
        "x_train = tf.random.uniform(shape=[train_size, 1], minval=x_min, maxval=x_max )\n",
        "x_test = tf.linspace(x_min, x_max, num=test_size)[:, tf.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxTntgziCLc"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9FMkQl1c63s"
      },
      "source": [
        "f1_trial = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "f2_trial = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "models = [f1_trial, f2_trial]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqNW61EAuwvp"
      },
      "source": [
        "f1_x_init = tf.constant([[0.0]])\n",
        "f2_x_init = tf.constant([[0.0]])\n",
        "f1_init_val = tf.constant([[0.0]])\n",
        "f2_init_val = tf.constant([[1.0]])\n",
        "true_init_vals = [f1_init_val, f2_init_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4cHL6q4lHPz"
      },
      "source": [
        "def loss_fn(inputs, input_grads, logits, init_vals):\n",
        "  ode_loss1 = tf.square(input_grads[0] - tf.math.cos(inputs) - logits[0]**2 - logits[1] + (1 + inputs**2 + tf.math.sin(inputs)**2))\n",
        "  init_loss1 = tf.square(init_vals[0] - true_init_vals[0])\n",
        "  ode_loss2 = tf.square(input_grads[1] - 2*inputs + (1 + inputs**2)*tf.math.sin(inputs) - logits[0]*logits[1])\n",
        "  init_loss2 = tf.square(init_vals[1] - true_init_vals[1])\n",
        "  loss1 = tf.reduce_sum(ode_loss1 + init_loss1)\n",
        "  loss2 = tf.reduce_sum(ode_loss2 + init_loss2)\n",
        "  return loss1 + loss2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG7BU6jCYtd-"
      },
      "source": [
        "def loss_val_grad(models, inputs):\n",
        "\n",
        "  shapes = [var for sub in [tf.shape_n(m.trainable_weights) for m in models] for var in sub]\n",
        "  n_tensors = len(shapes)\n",
        "\n",
        "  # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
        "  # prepare required information first\n",
        "  count = 0\n",
        "  idx = [] # stitch indices\n",
        "  part = [] # partition indices\n",
        "\n",
        "  for i, shape in enumerate(shapes):\n",
        "    n = np.product(shape)\n",
        "    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
        "    part.extend([i]*n)\n",
        "    count += n\n",
        "\n",
        "  part = tf.constant(part)\n",
        "\n",
        "  def update_params(params):\n",
        "    train_weights = [var for sub in [m.trainable_weights for m in models] for var in sub]\n",
        "    params_var = tf.dynamic_partition(params, part, n_tensors)\n",
        "    for i, (shape, param) in enumerate(zip(shapes, params_var)):\n",
        "        train_weights[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "  def func(params):\n",
        "    logits = []\n",
        "    input_grads = []\n",
        "    update_params(params)\n",
        "    init_vals = [models[0](f1_x_init), models[1](f2_x_init)]\n",
        "    with tf.GradientTape(persistent=True) as tp:\n",
        "      for model in models:\n",
        "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "          tape.watch(inputs)\n",
        "          logit = model(inputs)\n",
        "          logits.append(logit)\n",
        "        input_grads.append(tape.gradient(logit, inputs))\n",
        "      loss_val = loss_fn(inputs, input_grads, logits, init_vals)\n",
        "    grads = [tp.gradient(loss_val, m.trainable_weights) for m in models]\n",
        "    grads = sum(grads, [])\n",
        "    grads = tf.dynamic_stitch(idx, grads)\n",
        "    return loss_val, grads\n",
        "\n",
        "  func.idx = idx\n",
        "  func.part = part\n",
        "  func.shapes = shapes\n",
        "  func.update_params = update_params\n",
        "\n",
        "  return func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyr65W7yd_jJ"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f\"\\nStart of epoch {epoch}:\")\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    val_grad_func = loss_val_grad(models, x_batch_train)\n",
        "    lbfgs_init_pos = tf.dynamic_stitch(val_grad_func.idx, [var for sub in [m.trainable_weights for m in models] for var in sub])\n",
        "    optim_results = tfp.optimizer.lbfgs_minimize(val_grad_func, initial_position=lbfgs_init_pos, max_iterations=100)\n",
        "    val_grad_func.update_params(optim_results.position)\n",
        "    \n",
        "    # if step % 100 == 0:\n",
        "    print(f\"\\tTraining loss at step {step}: {optim_results.objective_value.numpy()}\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn_4s2XWgyqz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}