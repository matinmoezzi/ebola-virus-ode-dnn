{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neurodiffeq_example_lbfgs.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOaVlzyVssF0Gq7GIPYzPAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matinmoezzi/ebola-virus-ode-dnn/blob/main/neurodiffeq_example_lbfgs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FVPReRIWlDV"
      },
      "source": [
        "!pip install neurodiffeq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlaQghDeW1Vg"
      },
      "source": [
        "from itertools import chain\n",
        "from neurodiffeq import diff\n",
        "from neurodiffeq.conditions import IVP\n",
        "from neurodiffeq.monitors import Monitor1D\n",
        "from neurodiffeq.solvers import Solver1D\n",
        "from neurodiffeq.callbacks import MonitorCallback\n",
        "from neurodiffeq.networks import FCNN\n",
        "from neurodiffeq.generators import Generator1D\n",
        "from torch.optim import LBFGS\n",
        "import torch.autograd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO-7eNaDXurC"
      },
      "source": [
        "class MySolver(Solver1D):\n",
        "  def _run_epoch(self, key):\n",
        "        r\"\"\"Run an epoch on train/valid points, update history, and perform an optimization step if key=='train'.\n",
        "\n",
        "        :param key: {'train', 'valid'}; phase of the epoch\n",
        "        :type key: str\n",
        "\n",
        "        .. note::\n",
        "            The optimization step is only performed after all batches are run.\n",
        "        \"\"\"\n",
        "        self._phase = key\n",
        "        epoch_loss = 0.0\n",
        "        batch_loss = 0.0\n",
        "        metric_values = {name: 0.0 for name in self.metrics_fn}\n",
        "\n",
        "        # perform forward pass for all batches: a single graph is created and release in every iteration\n",
        "        # see https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/17\n",
        "        for batch_id in range(self.n_batches[key]):\n",
        "            batch = self._generate_batch(key)\n",
        "\n",
        "            def closure():\n",
        "                nonlocal batch_loss\n",
        "                if key == 'train':\n",
        "                    self.optimizer.zero_grad()\n",
        "                funcs = [\n",
        "                    self.compute_func_val(n, c, *batch) for n, c in zip(self.nets, self.conditions)\n",
        "                ]\n",
        "\n",
        "                for name in self.metrics_fn:\n",
        "                    value = self.metrics_fn[name](*funcs, *batch).item()\n",
        "                    metric_values[name] += value\n",
        "                residuals = self.diff_eqs(*funcs, *batch)\n",
        "                residuals = torch.cat(residuals, dim=1)\n",
        "                loss = self.criterion(residuals) + \\\n",
        "                    self.additional_loss(funcs, key)\n",
        "\n",
        "                # normalize loss across batches\n",
        "                # loss /= self.n_batches[key]\n",
        "\n",
        "                # accumulate gradients before the current graph is collected as garbage\n",
        "                if key == 'train':\n",
        "                    loss.backward()\n",
        "                    batch_loss = loss.item()\n",
        "                return loss\n",
        "                # epoch_loss += loss.item()\n",
        "            if key == 'train':\n",
        "                self._do_optimizer_step(closure=closure)\n",
        "                epoch_loss += batch_loss\n",
        "            else:\n",
        "                epoch_loss += closure().item()\n",
        "\n",
        "        # calculate mean loss of all batches and register to history\n",
        "        self._update_history(epoch_loss / self.n_batches[key], 'loss', key)\n",
        "\n",
        "        # perform optimization step when training\n",
        "        # self.optimizer.zero_grad()\n",
        "        # update lowest_loss and best_net when validating\n",
        "        if key == 'valid':\n",
        "            self._update_best()\n",
        "\n",
        "        # calculate average metrics across batches and register to history\n",
        "        for name in self.metrics_fn:\n",
        "            self._update_history(\n",
        "                metric_values[name] / self.n_batches[key], name, key)\n",
        "            \n",
        "  def _do_optimizer_step(self, closure=None):\n",
        "        r\"\"\"Optimization procedures after gradients have been computed. Usually ``self.optimizer.step()`` is sufficient.\n",
        "        At times, users can overwrite this method to perform gradient clipping, etc. Here is an example::\n",
        "\n",
        "            import itertools\n",
        "            class MySolver(Solver)\n",
        "                def _do_optimizer_step(self):\n",
        "                    nn.utils.clip_grad_norm_(itertools.chain([net.parameters() for net in self.nets]), 1.0, 'inf')\n",
        "                    self.optimizer.step()\n",
        "        \"\"\"\n",
        "        return self.optimizer.step(closure=closure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0D8JAUW4g4"
      },
      "source": [
        "def system_ode(u1, u2, t): return [diff(u1, t) - torch.cos(t) - u1**2 - u2 + (\n",
        "    1 + t**2 + torch.sin(t)**2), diff(u2, t) - 2*t + (1 + t**2)*torch.sin(t) - u1*u2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESL_gMyvW7DY"
      },
      "source": [
        "init_vals_pc = [\n",
        "    IVP(t_0=0.0, u_0=0.0),\n",
        "    IVP(t_0=0.0, u_0=1.0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHZBvWSW8IX"
      },
      "source": [
        "monitor = Monitor1D(t_min=0, t_max=3, check_every=10)\n",
        "monitor_callback = MonitorCallback(monitor)\n",
        "\n",
        "\n",
        "def my_callback(solver):\n",
        "    if solver.lowest_loss < 1e-6:\n",
        "        solver._stop_training = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NodXGQVyW977"
      },
      "source": [
        "nets_lv = [\n",
        "    FCNN(n_input_units=1, n_output_units=1,\n",
        "         n_hidden_units=10, actv=nn.Sigmoid),\n",
        "    FCNN(n_input_units=1, n_output_units=1,\n",
        "         n_hidden_units=10, actv=nn.Sigmoid)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc7bJzKBXCsz"
      },
      "source": [
        "lbfgs = LBFGS(chain.from_iterable(n.parameters()\n",
        "                                  for n in nets_lv), lr=0.01, max_iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86sdiIBpXFZs"
      },
      "source": [
        "solver = MySolver(\n",
        "    ode_system=system_ode, conditions=init_vals_pc, t_min=0.0, t_max=3.0, nets=nets_lv, optimizer=lbfgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QESxIVjvatKL"
      },
      "source": [
        "solver.fit(max_epochs=100, callbacks=[monitor_callback, my_callback])\n",
        "solution_pc = solver.get_solution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Tz-8RFXH07"
      },
      "source": [
        "ts = np.linspace(0, 3, 3000)\n",
        "y1, y2 = solution_pc(ts, to_numpy=True)\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "axes[0].plot(ts, y1, label='ANN solution')\n",
        "axes[0].plot(ts, np.sin(ts), label='analytical solution')\n",
        "axes[0].legend()\n",
        "axes[1].plot(ts, y2, label='ANN solution')\n",
        "axes[1].plot(ts, 1 + ts**2, label='analytical solution')\n",
        "axes[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-3pBlJObsMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}